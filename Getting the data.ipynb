{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Getting the data.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiKpAyvSacfo",
        "colab_type": "text"
      },
      "source": [
        "### Tutorial Overview\n",
        "\n",
        "This tutorial is divided into 5 parts; they are:\n",
        "\n",
        "1. Europarl Machine Translation Dataset\n",
        "2. Download French-English Dataset\n",
        "3. Load Dataset\n",
        "4. Clean Dataset\n",
        "5. Reduce Vocabulary\n",
        "\n",
        "### Python Environment\n",
        "\n",
        "This tutorial assumes you have a Python SciPy environment installed with Python 3 installed.\n",
        "\n",
        "The tutorial also assumes you have scikit-learn, Pandas, NumPy, and Matplotlib installed.\n",
        "\n",
        "### Xitsonga Language Dictionary Website\n",
        "\n",
        "This website has English to Xitsonga translations, so we will use this website to scrape the data that we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhOyAieTep3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eab00f33-2e95-4c7b-d0ff-fca3373d1865"
      },
      "source": [
        "import requests\n",
        "\n",
        "# Library for parsing HTML\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "links = [\n",
        "  'https://www.xitsonga.org/grammar/greeting', \n",
        "  'https://www.xitsonga.org/grammar/emotions', \n",
        "  'https://www.xitsonga.org/grammar/how-to-ask',\n",
        "  'https://www.xitsonga.org/grammar/phrases&page=1' #Pages range from 1-8\n",
        "]\n",
        "\n",
        "base_url = 'https://www.xitsonga.org/grammar/greeting'\n",
        "\n",
        "index = requests.get(base_url).text\n",
        "soup_index = BeautifulSoup(index, 'html.parser')\n",
        "\n",
        "# We know the data is displayed on a table, and the first column is the Tsonga mappings, the 2nd column being the English corrensponding translation\n",
        "rows = soup_index.find_all('tr')\n",
        "\n",
        "# Every row has data, but the Tsonga cols have their data in links\n",
        "# tsonga = [tr.find('td') for tr in rows]\n",
        "rows[:2]\n",
        "# table_body\n",
        "# dumps = [tr['href'] for tr in soup_index.find_all('dictionary_data_table') if \n",
        "#          a.has_attr('href')]\n",
        "\n",
        "# dumps"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tr><th>Xitsonga</th><th>English</th></tr>,\n",
              " <tr><td style=\"width:40%\"><a href=\"grammar/greeting?_=baloyi\">Baloyi</a></td><td>-</td></tr>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXQg5lq3acfq",
        "colab_type": "text"
      },
      "source": [
        "### Download French-English Dataset\n",
        "\n",
        "We will focus on the parallel French-English dataset.\n",
        "\n",
        "This is a prepared corpus of aligned French and English sentences recorded between 1996 and 2011.\n",
        "\n",
        "The dataset has the following statistics:\n",
        "\n",
        "<li>Sentences: 2,007,723</li>\n",
        "<li>French words: 51,388,643</li>\n",
        "<li>English words: 50,196,035</li>\n",
        "\n",
        "You can download the dataset from here:\n",
        "\n",
        "<a href=\"http://www.statmt.org/europarl/v7/fr-en.tgz\">Parallel corpus French-English</a> (194 Megabytes)\n",
        "Once downloaded, you should have the file “fr-en.tgz” in your current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_BKeNTaacfr",
        "colab_type": "code",
        "colab": {},
        "outputId": "4d2b5c02-2a53-4ad3-c0a7-ece46f1f470b"
      },
      "source": [
        "#Download the Corpus\n",
        "!wget http://www.statmt.org/europarl/v7/fr-en.tgz -O fr-en.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-02 16:27:37--  http://www.statmt.org/europarl/v7/fr-en.tgz\n",
            "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
            "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 202718517 (193M) [application/x-gzip]\n",
            "Saving to: ‘fr-en.tgz’\n",
            "\n",
            "fr-en.tgz           100%[===================>] 193.33M   121KB/s    in 16m 51s \n",
            "\n",
            "2019-12-02 16:44:29 (196 KB/s) - ‘fr-en.tgz’ saved [202718517/202718517]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf6KFTzEacfv",
        "colab_type": "code",
        "colab": {},
        "outputId": "6054759f-096e-4cee-da64-17e594f9ed46"
      },
      "source": [
        "# Once downloaded, you should have the file “fr-en.tgz” in your current working directory.\n",
        "\n",
        "# You can unzip this archive file using the tar command, as follows:\n",
        "!tar zxvf fr-en.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "europarl-v7.fr-en.en\n",
            "europarl-v7.fr-en.fr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7VYKaRfacfx",
        "colab_type": "text"
      },
      "source": [
        "#### You will now have two files, as follows:\n",
        "\n",
        "<p></p>\n",
        "<li>English: europarl-v7.fr-en.en (288M)</li>\n",
        "<li>French: europarl-v7.fr-en.fr (331M)</li>\n",
        "\n",
        "Below is a sample of the English file.\n",
        "\n",
        "```\n",
        "Resumption of the session\n",
        "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
        "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
        "You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
        "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
        "```\n",
        "\n",
        "Below is a sample of the French file.\n",
        "\n",
        "```\n",
        "Reprise de la session\n",
        "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
        "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
        "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.\n",
        "En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-N6qXFzacfy",
        "colab_type": "text"
      },
      "source": [
        "### Load Dataset\n",
        "Let’s start off by loading the data files.\n",
        "\n",
        "We can load each file as a string. Because the files contain unicode characters, we must specify an encoding when loading the files as text. In this case, we will use UTF-8 that will easily handle the unicode characters in both files.\n",
        "\n",
        "The function below, named `` load_doc() ``, will load a given file and return it as a blob of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om8QCE4Qacfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Doc into memory \n",
        "\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    \n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    \n",
        "    # close the file\n",
        "    file.close()\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6NzX14Wacf1",
        "colab_type": "text"
      },
      "source": [
        "Next, we can split the file into sentences.\n",
        "\n",
        "Generally, one utterance is stored on each line. We can treat these as sentences and split the file by new line characters. The function `` to_sentences() `` below will split a loaded document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbfgt6ZGacf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split a loaded document inot sentences \n",
        "\n",
        "def to_sentences(doc):\n",
        "    return doc.strip().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrEBYsGQacf3",
        "colab_type": "text"
      },
      "source": [
        "When preparing our model later, we will need to know the length of sentences in the dataset. We can write a short function to calculate the shortest and longest sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCOgtKfMacf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shortest and Longest sentence lengths\n",
        "def sentence_lengths(sentences):\n",
        "    lengths = [len(s.split()) for s in sentences]\n",
        "    return min(lengths), max(lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79IYXBcnacf6",
        "colab_type": "text"
      },
      "source": [
        "We can tie all of this together to load and summarize the English and French data files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wSXDyQmacf7",
        "colab_type": "code",
        "colab": {},
        "outputId": "3141c5fc-d674-4e94-eeb1-8d7932a57639"
      },
      "source": [
        "# load English data\n",
        "filename = 'europarl-v7.fr-en.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        " \n",
        "# load French data\n",
        "filename = 'europarl-v7.fr-en.fr'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('French data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English data: sentences=2007723, min=0, max=668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUxOfq2Facf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}