{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial Overview\n",
    "\n",
    "This tutorial is divided into 5 parts; they are:\n",
    "\n",
    "1. Europarl Machine Translation Dataset\n",
    "2. Download French-English Dataset\n",
    "3. Load Dataset\n",
    "4. Clean Dataset\n",
    "5. Reduce Vocabulary\n",
    "\n",
    "### Python Environment\n",
    "\n",
    "This tutorial assumes you have a Python SciPy environment installed with Python 3 installed.\n",
    "\n",
    "The tutorial also assumes you have scikit-learn, Pandas, NumPy, and Matplotlib installed.\n",
    "\n",
    "### Europarl Machine Translation Dataset\n",
    "\n",
    "The Europarl is a standard dataset used for statistical machine translation, and more recently, neural machine translation.\n",
    "\n",
    "It is comprised of the proceedings of the European Parliament, hence the name of the dataset as the contraction Europarl.\n",
    "\n",
    "The proceedings are the transcriptions of speakers at the European Parliament, which are translated into 11 different languages.\n",
    "\n",
    "\"It is a collection of the proceedings of the European Parliament, dating back to 1996. Altogether, the corpus comprises of about 30 million words for each of the 11 official languages of the European Union \"\n",
    "\n",
    "— Europarl: A Parallel Corpus for Statistical Machine Translation, 2005.\n",
    "\n",
    "The raw data is available on the European Parliament website in HTML format.\n",
    "\n",
    "The creation of the dataset was lead by Philipp Koehn, author of the book “Statistical Machine Translation.”\n",
    "\n",
    "The dataset was made available for free to researchers on the website “European Parliament Proceedings Parallel Corpus 1996-2011,” and often appears as a part of machine translation challenges, such as the Machine Translation task in the 2014 Workshop on Statistical Machine Translation.\n",
    "\n",
    "The most recent version of the dataset is version 7, released in 2012, comprised of data from 1996 to 2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download French-English Dataset\n",
    "\n",
    "We will focus on the parallel French-English dataset.\n",
    "\n",
    "This is a prepared corpus of aligned French and English sentences recorded between 1996 and 2011.\n",
    "\n",
    "The dataset has the following statistics:\n",
    "\n",
    "<li>Sentences: 2,007,723</li>\n",
    "<li>French words: 51,388,643</li>\n",
    "<li>English words: 50,196,035</li>\n",
    "\n",
    "You can download the dataset from here:\n",
    "\n",
    "<a href=\"http://www.statmt.org/europarl/v7/fr-en.tgz\">Parallel corpus French-English</a> (194 Megabytes)\n",
    "Once downloaded, you should have the file “fr-en.tgz” in your current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-12-02 16:27:37--  http://www.statmt.org/europarl/v7/fr-en.tgz\n",
      "Resolving www.statmt.org (www.statmt.org)... 129.215.197.184\n",
      "Connecting to www.statmt.org (www.statmt.org)|129.215.197.184|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 202718517 (193M) [application/x-gzip]\n",
      "Saving to: ‘fr-en.tgz’\n",
      "\n",
      "fr-en.tgz           100%[===================>] 193.33M   121KB/s    in 16m 51s \n",
      "\n",
      "2019-12-02 16:44:29 (196 KB/s) - ‘fr-en.tgz’ saved [202718517/202718517]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download the Corpus\n",
    "!wget http://www.statmt.org/europarl/v7/fr-en.tgz -O fr-en.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europarl-v7.fr-en.en\n",
      "europarl-v7.fr-en.fr\n"
     ]
    }
   ],
   "source": [
    "# Once downloaded, you should have the file “fr-en.tgz” in your current working directory.\n",
    "\n",
    "# You can unzip this archive file using the tar command, as follows:\n",
    "!tar zxvf fr-en.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will now have two files, as follows:\n",
    "\n",
    "<p></p>\n",
    "<li>English: europarl-v7.fr-en.en (288M)</li>\n",
    "<li>French: europarl-v7.fr-en.fr (331M)</li>\n",
    "\n",
    "Below is a sample of the English file.\n",
    "\n",
    "```\n",
    "Resumption of the session\n",
    "I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
    "Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
    "You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
    "In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
    "```\n",
    "\n",
    "Below is a sample of the French file.\n",
    "\n",
    "```\n",
    "Reprise de la session\n",
    "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n",
    "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.\n",
    "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.\n",
    "En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "Let’s start off by loading the data files.\n",
    "\n",
    "We can load each file as a string. Because the files contain unicode characters, we must specify an encoding when loading the files as text. In this case, we will use UTF-8 that will easily handle the unicode characters in both files.\n",
    "\n",
    "The function below, named `` load_doc() ``, will load a given file and return it as a blob of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Doc into memory \n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    \n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    \n",
    "    # close the file\n",
    "    file.close()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can split the file into sentences.\n",
    "\n",
    "Generally, one utterance is stored on each line. We can treat these as sentences and split the file by new line characters. The function `` to_sentences() `` below will split a loaded document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a loaded document inot sentences \n",
    "\n",
    "def to_sentences(doc):\n",
    "    return doc.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When preparing our model later, we will need to know the length of sentences in the dataset. We can write a short function to calculate the shortest and longest sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest and Longest sentence lengths\n",
    "def sentence_lengths(sentences):\n",
    "    lengths = [len(s.split()) for s in sentences]\n",
    "    return min(lengths), max(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tie all of this together to load and summarize the English and French data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English data: sentences=2007723, min=0, max=668\n"
     ]
    }
   ],
   "source": [
    "# load English data\n",
    "filename = 'europarl-v7.fr-en.en'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "minlen, maxlen = sentence_lengths(sentences)\n",
    "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
    " \n",
    "# load French data\n",
    "filename = 'europarl-v7.fr-en.fr'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "minlen, maxlen = sentence_lengths(sentences)\n",
    "print('French data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
